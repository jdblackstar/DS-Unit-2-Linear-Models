{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint Challenge Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Concepts:\n",
    "* We want to minimzie the square of the distance between the data and the line\n",
    "* We do this by taking the derivative and finding where it's equal to 0\n",
    "* Variance(something) = Sums of squares/number of things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression \n",
    "\n",
    "## Main Ideas\n",
    "* Use least-squares to fit a line to the data\n",
    "* Calculate R^2\n",
    "* Canculate the p-value for R^2\n",
    "\n",
    "### Steps for Least Squares\n",
    "* Draw a line through the data\n",
    "* Second, measure the distance between each point and the line (residuals)\n",
    "* Rotate the line a little bit and see how the sum of residuals changes\n",
    "* Repeat\n",
    "* Repeat\n",
    "* Find the rotation that has the least sum of squares\n",
    "\n",
    "### Calculate R^2\n",
    "* shift all points to the y-axis\n",
    "* find the average mouse size (y-value)\n",
    "* sum square residuals: measure distance from point to line (SS(mean))\n",
    "* find SS(fit), which is the sum of squares for the whole line\n",
    "* residuals of SS(fit) are smaller than those of SS(mean), so there is some correlation\n",
    "\n",
    "R^2 = var(mean) - var(fit) / var(mean)\n",
    "\n",
    "\"There is an [R^2]% reduction in variance when we take mouse weight into account.\"\n",
    "\"Mouse weight explains 60% of variation in mouse size.\"\n",
    "\"60% of the mouse size can be explained by the mouse weight.\"\n",
    "\n",
    "In a 3-axis graph, if one of the axis doesn't make SS(fit) smaller, then least-squares will be 0.\n",
    "This means that an equation with multiple parameters will NEVER perform worse than an equation with a single variable. The more parameters we add to the equation, the more opportunities we have for random events to reduce SS(fit) and result ina  better R2 value (adjusted R2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if all we had were two measurements?\n",
    "* SS(mean) and SS(fit) = 0, because you can always draw a straight line to calculate any two points, but you can not say that x explains y\n",
    "* we need a p-value to make sure that the R2 value is significant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias and Variance\n",
    "\n",
    "### Terminology\n",
    "* Bias: inability for a regression model to capture the true relationship (high sum of squares on training set)\n",
    "* Variance: the difference in fits between data sets\n",
    "* Linear Regression - \"Least Squares\"\n",
    "    - Will never curve, no matter how well we fit it to the training set\n",
    "    - Relatively large amount of bias, cannot capture relationship between 2 things\n",
    "    - Low variance\n",
    "    - Not great predictions, but consistent predictions\n",
    "* Squiggly Line \n",
    "    - Relatively low amount of bias, flexible and can adapt to curve\n",
    "    - High variability, because it can adapt to curve\n",
    "* Overfit\n",
    "    - a model that fits the training set really well, but not the testing set\n",
    "    - low bias, high variance\n",
    "    - 'forcefitting', too good to be true\n",
    "* Underfit\n",
    "    - a model that is too simple to explain the variance\n",
    "\n",
    "\n",
    "### Reminders\n",
    "* I can compare how well the Straight Line and Squiggly Line fit the training set by calculating their sums of square \n",
    "    - they are squared so negative distances don't cancel out positive distances \n",
    "* The IDEAL model has low bias and low variability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P-value in regression\n",
    "\n",
    "### Main Ideas\n",
    "* R2 = SS(mean) - SS(fit) / SS(mean)\n",
    "* F = (SS(mean) - SS(fit) / (Pfit - Pmean)) / (SS(fit) / (n - Pfit))\n",
    "* Pfit is the number of parameters in the fit line\n",
    "* Pmean is the number of parameters in the fit line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "\n",
    "### Terminology\n",
    "\n",
    "\n",
    "### What is Ridge Regression?\n",
    "* Main Idea is to find a new line that doesn't fit the training data as well (add bias), but in return we get a significant drop in variance (fits the testing data better)\n",
    "* By starting with a slightly worse fit, ridge regression can provide better long term predictions\n",
    "* The point is always to minimize bias and variance\n",
    "* works for continuous x continuous data as well as categorical x continuous\n",
    "\n",
    "\n",
    "### Least Squares vs. Ridge Regression\n",
    "* Least Squares\n",
    "    - when determining values for the parameters for the equation of the line, least squares minimizes the of the squared residuals\n",
    "* Ridge Regression\n",
    "    - when determining values for the parameters for the equation of the line, ridge regression minimizes the of the squared residuals + $\\lambda * slope**\n",
    "    - this new equation adds a penalty to the traditional Least Squares method, and $\\lambda determines how severe that penalty is\n",
    "* When to use LS vs. RR \n",
    "    - You would choose the line that has the minimized sum of the squared residuals plus the ridge regression penalty ($\\lambda)\n",
    "\n",
    "### Lambda \n",
    "* any number from negative to positive infinity\n",
    "* larger we make lambda, the slope get asymptotically close to 0\n",
    "* How do we decide what value to give to lambda?\n",
    "    - try a bunch of values and use cross validation (typically 10-fold cross validation) and find the one that results in the lowest variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}